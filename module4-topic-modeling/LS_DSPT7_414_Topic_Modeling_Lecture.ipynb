{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cOMBkFKn6uEK"
   },
   "source": [
    "# Topic Modeling (Prepare)\n",
    "\n",
    "On Monday we talked about summarizing your documents using just token counts. Today, we're going to learn about a much more sophisticated approach - learning 'topics' from documents. Topics are a latent structure. They are not directly observable in the data, but we know they're there by reading them.\n",
    "\n",
    "> **latent**: existing but not yet developed or manifest; hidden or concealed.\n",
    "\n",
    "## Use Cases\n",
    "Primary use case: what the hell are your documents about? Who might want to know that in industry - \n",
    "* Identifying common themes in customer reviews\n",
    "* Discovering the needle in a haystack \n",
    "* Monitoring communications (Email - State Department) \n",
    "\n",
    "## Learning Objectives\n",
    "*At the end of the lesson you should be able to:*\n",
    "* Part 0: Warm-Up\n",
    "* Part 1: Describe how an LDA Model works\n",
    "* Part 2: Estimate a LDA Model with Gensim\n",
    "* Part 3: Interpret LDA results & Select the appropriate number of topics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cxH0cRGX6uEL"
   },
   "source": [
    "# Part 0: Warm-Up\n",
    "How do we do a grid search? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FFjplZ7r3Lyk"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rnyNg8sQ6uEL"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "boKWVkRz6uEO"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Samples: 11314\n",
      "Testing Samples: 7532\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "newsgroups_train = fetch_20newsgroups(subset='train', \n",
    "                                      remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Load testing data\n",
    "newsgroups_test = fetch_20newsgroups(subset='test', \n",
    "                                     remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "print(f'Training Samples: {len(newsgroups_train.data)}')\n",
    "print(f'Testing Samples: {len(newsgroups_test.data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hPpukADi7Ofv"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g9Jhb3j-7_e2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['alt.atheism',\n",
       " 'comp.graphics',\n",
       " 'comp.os.ms-windows.misc',\n",
       " 'comp.sys.ibm.pc.hardware',\n",
       " 'comp.sys.mac.hardware',\n",
       " 'comp.windows.x',\n",
       " 'misc.forsale',\n",
       " 'rec.autos',\n",
       " 'rec.motorcycles',\n",
       " 'rec.sport.baseball',\n",
       " 'rec.sport.hockey',\n",
       " 'sci.crypt',\n",
       " 'sci.electronics',\n",
       " 'sci.med',\n",
       " 'sci.space',\n",
       " 'soc.religion.christian',\n",
       " 'talk.politics.guns',\n",
       " 'talk.politics.mideast',\n",
       " 'talk.politics.misc',\n",
       " 'talk.religion.misc']"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train['target_names']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JbVoIoMm8HbR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Anybody seen mouse cursor distortion running the Diamond 1024x768x256 driver?\\nSorry, don't know the version of the driver (no indication in the menus) but it's a recently\\ndelivered Gateway system.  Am going to try the latest drivers from Diamond BBS but wondered\\nif anyone else had seen this.\\n\\npost or email\""
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train['data'][1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "w-39zg9a6uEQ"
   },
   "source": [
    "### GridSearch on Just Classifier\n",
    "* Fit the vectorizer and prepare BEFORE it goes into the gridsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2q3NmHEo6uEQ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 101631)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate vectorizer\n",
    "vect = TfidfVectorizer()\n",
    "\n",
    "# Transform the training data\n",
    "X_train = vect.fit_transform(newsgroups_train['data'])\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7OfA7KNQ6uEU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 out of  20 | elapsed:  1.2min remaining:    8.1s\n",
      "[Parallel(n_jobs=-1)]: Done  20 out of  20 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=RandomForestClassifier(bootstrap=True, ccp_alpha=0.0,\n",
       "                                              class_weight=None,\n",
       "                                              criterion='gini', max_depth=None,\n",
       "                                              max_features='auto',\n",
       "                                              max_leaf_nodes=None,\n",
       "                                              max_samples=None,\n",
       "                                              min_impurity_decrease=0.0,\n",
       "                                              min_impurity_split=None,\n",
       "                                              min_samples_leaf=1,\n",
       "                                              min_samples_split=2,\n",
       "                                              min_weight_fraction_leaf=0.0,\n",
       "                                              n_estimators=100, n_jobs=None,\n",
       "                                              oob_score=False,\n",
       "                                              random_state=None, verbose=0,\n",
       "                                              warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'min_samples_leaf': [1, 2, 5, 10]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_1 = {\n",
    "    'min_samples_leaf': [1, 2, 5, 10]\n",
    "}\n",
    "\n",
    "# Instantiate classifier\n",
    "clf = RandomForestClassifier()\n",
    "\n",
    "# GridSearch\n",
    "gs1 = GridSearchCV(clf, params_1, cv=5, n_jobs=-1, verbose=1)\n",
    "gs1.fit(X_train, newsgroups_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6557360498512768"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs1.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'min_samples_leaf': 2}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs1.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gDkMaXvG83Zt"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 101631)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_sample = vect.transform([\"The new york yankees are the best team in the region.\"])\n",
    "test_sample.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cMq_hw5W6uEX"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs1.predict(test_sample)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rec.sport.baseball'"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train['target_names'][9]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WdVTuXKl6uEZ"
   },
   "source": [
    "### GridSearch with BOTH the Vectoizer & Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "97TjUtoI6uEZ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 12 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  40 out of  40 | elapsed:  1.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score=nan,\n",
       "             estimator=Pipeline(memory=None,\n",
       "                                steps=[('vect',\n",
       "                                        TfidfVectorizer(analyzer='word',\n",
       "                                                        binary=False,\n",
       "                                                        decode_error='strict',\n",
       "                                                        dtype=<class 'numpy.float64'>,\n",
       "                                                        encoding='utf-8',\n",
       "                                                        input='content',\n",
       "                                                        lowercase=True,\n",
       "                                                        max_df=1.0,\n",
       "                                                        max_features=None,\n",
       "                                                        min_df=1,\n",
       "                                                        ngram_range=(1, 1),\n",
       "                                                        norm='l2',\n",
       "                                                        preprocessor=None,\n",
       "                                                        smooth_idf=True,\n",
       "                                                        stop_words=None,\n",
       "                                                        strip_acce...\n",
       "                                                               min_samples_split=2,\n",
       "                                                               min_weight_fraction_leaf=0.0,\n",
       "                                                               n_estimators=100,\n",
       "                                                               n_jobs=None,\n",
       "                                                               oob_score=False,\n",
       "                                                               random_state=42,\n",
       "                                                               verbose=0,\n",
       "                                                               warm_start=False))],\n",
       "                                verbose=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'clf__max_depth': (10, None), 'vect__min_df': (2, 5),\n",
       "                         'vect__stop_words': (None, 'english')},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring=None, verbose=1)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# 1. Create a pipeline with a vectorize and a classifier\n",
    "# 2. Use Grid Search to optimize the entire pipeline\n",
    "pipe = Pipeline([\n",
    "    ('vect',TfidfVectorizer()),\n",
    "    ('clf',RandomForestClassifier(random_state=42))\n",
    "])\n",
    "\n",
    "params_2 = {\n",
    "    'vect__stop_words': (None,'english'),\n",
    "    'vect__min_df': (2,5),\n",
    "    'clf__max_depth': (10, None)\n",
    "}\n",
    "\n",
    "gs2 = GridSearchCV(pipe, params_2, cv=5, n_jobs=-1, verbose=1)\n",
    "gs2.fit(newsgroups_train['data'], newsgroups_train['target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6607746264533867"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__max_depth': None, 'vect__min_df': 2, 'vect__stop_words': 'english'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IdIE67Us6uEc"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9])"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = gs2.predict([\"The new york yankees are the best team in the region.\"])\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xhcb5G0W-Dch"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rec.sport.baseball'"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newsgroups_train['target_names'][pred[0]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F2R2ACd36uEd"
   },
   "source": [
    "Advantages to using GS with the Pipe:\n",
    "* Allows us to make predictions on raw text increasing reproducibility. :)\n",
    "* Allows us to tune the parameters of the vectorizer along side the classifier. :D "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xbZjG6U86uEe"
   },
   "source": [
    "# Part 1: Describe how an LDA Model works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dMzU-XOM3LzW"
   },
   "source": [
    "[Your Guide to Latent Dirichlet Allocation](https://medium.com/@lettier/how-does-lda-work-ill-explain-using-emoji-108abf40fa7d)\n",
    "\n",
    "[LDA Topic Modeling](https://lettier.com/projects/lda-topic-modeling/)\n",
    "\n",
    "[Topic Modeling with Gensim](https://www.machinelearningplus.com/nlp/topic-modeling-gensim-python/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yPIlE_IeF0cX"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ” Download and installation successful\n",
      "You can now load the model via spacy.load('en_core_web_lg')\n"
     ]
    }
   ],
   "source": [
    "# Download spacy model\n",
    "import spacy.cli\n",
    "spacy.cli.download(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qapChu_UGBFc"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel\n",
    "\n",
    "import spacy\n",
    "import pyLDAvis\n",
    "import pyLDAvis.gensim \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qaMsy1XAGLxc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11314, 3)\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'content': newsgroups_train['data'],\n",
    "    'target': newsgroups_train['target'],\n",
    "    'target_names': [newsgroups_train['target_names'][i] for i in newsgroups_train['target']]\n",
    "})\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>\\nMaybe...then again did you get rid of that H/D of yorn and buy a rice rocket \\nof your own?  That would certainly explain the friendliness...unless you \\nmaybe had a piece of toilet paper stuck on the bottom of your boot...8-).\\n\\nRich\\n</td>\n",
       "      <td>8</td>\n",
       "      <td>rec.motorcycles</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>804</th>\n",
       "      <td>It worked!!!\\nThank you very much!\\n</td>\n",
       "      <td>3</td>\n",
       "      <td>comp.sys.ibm.pc.hardware</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9779</th>\n",
       "      <td>\\nJim, please, that's a lame explanation of the trinity that Jesus provides\\nabove. Baptizing people in the name of three things != trinity. If\\nthis is the case, then I'm wrong, I assumed that trinity implies that\\nGod is three entities, and yet the same.\\n\\nCheers,\\nKent</td>\n",
       "      <td>19</td>\n",
       "      <td>talk.religion.misc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                content  \\\n",
       "2235  \\nMaybe...then again did you get rid of that H/D of yorn and buy a rice rocket \\nof your own?  That would certainly explain the friendliness...unless you \\nmaybe had a piece of toilet paper stuck on the bottom of your boot...8-).\\n\\nRich\\n                                     \n",
       "804   It worked!!!\\nThank you very much!\\n                                                                                                                                                                                                                                                \n",
       "9779  \\nJim, please, that's a lame explanation of the trinity that Jesus provides\\nabove. Baptizing people in the name of three things != trinity. If\\nthis is the case, then I'm wrong, I assumed that trinity implies that\\nGod is three entities, and yet the same.\\n\\nCheers,\\nKent   \n",
       "\n",
       "      target              target_names  \n",
       "2235  8       rec.motorcycles           \n",
       "804   3       comp.sys.ibm.pc.hardware  \n",
       "9779  19      talk.religion.misc        "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.max_colwidth', 0)\n",
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "na2bkOcFGter"
   },
   "outputs": [],
   "source": [
    "# For reference on regex: https://docs.python.org/3/library/re.html\n",
    "\n",
    "# From 'content' column: \n",
    "\n",
    "# 1. Remove new line characters\n",
    "df['clean_text'] = df['content'].apply(lambda x: re.sub('\\s+', ' ', x))\n",
    "# 2. Remove extra whitespace \n",
    "df['clean_text'] = df['clean_text'].apply(lambda x: ' '.join(x.split()))\n",
    "# 3. Remove Emails\n",
    "df['clean_text'] = df['clean_text'].apply(lambda x: re.sub('From: \\S+@\\S+', '', x))\n",
    "# 4. Remove non-alphanumeric characters\n",
    "df['clean_text'] = df['clean_text'].apply(lambda x: re.sub('[^a-zA-Z]', ' ', x))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2prKILFo3Lzg"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8535</th>\n",
       "      <td>\\nYou could take my wrongly spelled surname :-).\\n\\nCheers,\\nKent Sandvik</td>\n",
       "      <td>0</td>\n",
       "      <td>alt.atheism</td>\n",
       "      <td>You could take my wrongly spelled surname      Cheers  Kent Sandvik</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5114</th>\n",
       "      <td>Last week I asked for help in getting an old homemade amp working with\\nmy Sun CD-ROM drive. It turns out that the channel I was testing with\\nwas burned out in the amp. The other channel works fine.\\n\\nSo now I need a new amplifier chip. My local Radio Shack no longer\\ncarries components! The chip is a 12 pin SIP (?) labelled with BA5406\\nand then \"502 515\" below that.\\n\\nDoes anyone have a source? Thanks,</td>\n",
       "      <td>12</td>\n",
       "      <td>sci.electronics</td>\n",
       "      <td>Last week I asked for help in getting an old homemade amp working with my Sun CD ROM drive  It turns out that the channel I was testing with was burned out in the amp  The other channel works fine  So now I need a new amplifier chip  My local Radio Shack no longer carries components  The chip is a    pin SIP     labelled with BA     and then           below that  Does anyone have a source  Thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5913</th>\n",
       "      <td>\\n\\nThe theory is that the hollering kills the spirit of the criminal/Nazi \\nArmenians of the ASALA/SDPA/ARF Terrorism and Revisionism Triangle. \\nNow, try dealing with the rest of what I wrote.\\n\\nWhat is more, the activities of the Armenian Government seem to have been\\nefforts aimed at eradicating a race (the Turks) or aimed at carrying out a\\none-sided feud, instead of being a struggle for liberation. From the outset,\\nthe efforts of the Armenian revolutionaries within the Ottoman borders took\\nthe form of terrorist and destructive actions aimed at mass murder, cruelty\\nand genocide, so that no other interpretation of them is possible. Armenian\\nactivities started during the reign of Abdulhamid II as individual acts of\\nterror, and then developed into assassinations and surprise attacks. The element\\nof brute force in these activities increased steadily, culminating in mass\\nrebellions and widespread fighting during the First World War. Furthermore,\\nwhen the Ottoman army withdrew from Eastern Anatolia after the 1915 Sarikamis\\ndefeat, Armenian revolutionaries initiated a series of cruelties in this area.\\nAlthough the Russians occupied Eastern Anatolia as an enemy, nevertheless they\\nwere constrained by the rules of war. However, when they returned to their\\ncountry in 1917 after the Revolution, Armenian revolutionaries were unchecked\\nin this area for about a year until the Ottoman forces returned to Erzurum\\nin 1918. During this period, Armenian revolutionaries executed massacres on\\nthe local people which is recorded in historical documents.[1]\\n\\nFor example, let us look at a report dated 21 March 1918 which the Commander\\nof the Third Army submitted when he entered Erzurum and Erzincan: \\n\\n \"They were completely and systematically destroyed and burned down \\n  by Armenians, even the trees were cut down, and they are like a \\n  building entirely consumed by fire in every sense of the word.\" \\n\\nAs for the people who had been living in Erzurum and Erzincan:\\n\\n\"Those who were capable of fighting were taken away at the very beginning\\n with the excuse of forced labor in road construction, they were taken\\n in the direction of Sarikamis and annihilated. When the Russian army\\n withdrew, a part of the remaining people was destroyed in Armenian\\n massacres and cruelties: they were thrown into wells, they were locked\\n in houses and burned down, they were killed with bayonets and swords, in places\\n selected as butchering spots, their bellies were torn open, their lungs\\n were pulled out, and girls and women were hanged by their hair after\\n being subjected to every conceivable abominable act. A very small part \\n of the people who were spared these abominations far worse than the\\n cruelty of the inquisition resembled living dead and were suffering\\n from temporary insanity because of the dire poverty they had lived\\n in and because of the frightful experiences they had been subjected to.\\n Including women and children, such persons discovered so far do not\\n exceed one thousand five hundred in Erzincan and thirty thousand in\\n Erzurum. All the fields in Erzincan and Erzurum are untilled, everything\\n that the people had has been taken away from them, and we found them\\n in a destitute situation. At the present time, the people are subsisting\\n on some food they obtained, impelled by starvation, from Russian storages\\n left behind after their occupation of this area.\"[2]\\n \\nForeign observers who witnessed the events, including Russian Officers\\nwho did not desert their lines, submitted detailed reports proving the\\ngenocide to Ottoman commanders who received them as prisoners of war.\\nWhat is most important is that they stated in their reports 'the \\nmassacres did not happen by chance but were planned.'[3]\\n\\nAt the end of the war, the German author Dr. Weiss, his Austrian colleague\\nDr. Stein and his Turkish colleague Mr. Ahmet Vefik visited Trabzon, Kars,\\nErzurum and Batum between April 17th and May 20th 1918 to record the\\ncruelties. Their writings not only show the scope of Armenian activities,\\nbut also reveal their goal and true nature.[4]\\n\\n[1] (The Ottoman State, the Ministry of War), \"Islam Ahalinin Ducar Olduklari\\n    Mezalim Hakkinda Vesaike Mustenid Malumat,\" (Istanbul, 1918). The French\\n    version: \"Documents Relatifs aux Atrocites Commises par les Armeniens sur\\n    la Population Musulmane,\" (Istanbul, 1919). In the Latin script: H. K.\\n    Turkozu, ed., \"Osmanli ve Sovyet Belgeleriyle Ermeni Mezalimi,\" (Ankara,\\n    1982). In addition: Z. Basar, ed., \"Ermenilerden Gorduklerimiz,\" (Ankara,\\n    1974) and, edited by the same author, \"Ermeniler Hakkinda Makaleler -\\n    Derlemeler,\" (Ankara, 1978). \"Askeri Tarih Belgeleri ...,\" Vol. 32, 83\\n    (December 1983), document numbered 1881.\\n[2] \"Askeri Tarih Belgeleri ....,\" Vol. 31, 81 (December 1982), document\\n    numbered 1869.\\n[3] From Twerdo-Khlebof's report dated 29 April 1918; quoted in Ermeniler ...,\\n    Vol. 2, p. 275.\\n[4] A. R. (Altinay), \"Iki Komite - Iki Kital,\" (Istanbul, 1919), and, \"Kafkas\\n    Yollarinda Hatiralar ve Tahassusler\" (Istanbul, 1919).\\n\\n\\nSerdar Argic</td>\n",
       "      <td>17</td>\n",
       "      <td>talk.politics.mideast</td>\n",
       "      <td>The theory is that the hollering kills the spirit of the criminal Nazi Armenians of the ASALA SDPA ARF Terrorism and Revisionism Triangle  Now  try dealing with the rest of what I wrote  What is more  the activities of the Armenian Government seem to have been efforts aimed at eradicating a race  the Turks  or aimed at carrying out a one sided feud  instead of being a struggle for liberation  From the outset  the efforts of the Armenian revolutionaries within the Ottoman borders took the form of terrorist and destructive actions aimed at mass murder  cruelty and genocide  so that no other interpretation of them is possible  Armenian activities started during the reign of Abdulhamid II as individual acts of terror  and then developed into assassinations and surprise attacks  The element of brute force in these activities increased steadily  culminating in mass rebellions and widespread fighting during the First World War  Furthermore  when the Ottoman army withdrew from Eastern Anatolia after the      Sarikamis defeat  Armenian revolutionaries initiated a series of cruelties in this area  Although the Russians occupied Eastern Anatolia as an enemy  nevertheless they were constrained by the rules of war  However  when they returned to their country in      after the Revolution  Armenian revolutionaries were unchecked in this area for about a year until the Ottoman forces returned to Erzurum in       During this period  Armenian revolutionaries executed massacres on the local people which is recorded in historical documents     For example  let us look at a report dated    March      which the Commander of the Third Army submitted when he entered Erzurum and Erzincan   They were completely and systematically destroyed and burned down by Armenians  even the trees were cut down  and they are like a building entirely consumed by fire in every sense of the word   As for the people who had been living in Erzurum and Erzincan   Those who were capable of fighting were taken away at the very beginning with the excuse of forced labor in road construction  they were taken in the direction of Sarikamis and annihilated  When the Russian army withdrew  a part of the remaining people was destroyed in Armenian massacres and cruelties  they were thrown into wells  they were locked in houses and burned down  they were killed with bayonets and swords  in places selected as butchering spots  their bellies were torn open  their lungs were pulled out  and girls and women were hanged by their hair after being subjected to every conceivable abominable act  A very small part of the people who were spared these abominations far worse than the cruelty of the inquisition resembled living dead and were suffering from temporary insanity because of the dire poverty they had lived in and because of the frightful experiences they had been subjected to  Including women and children  such persons discovered so far do not exceed one thousand five hundred in Erzincan and thirty thousand in Erzurum  All the fields in Erzincan and Erzurum are untilled  everything that the people had has been taken away from them  and we found them in a destitute situation  At the present time  the people are subsisting on some food they obtained  impelled by starvation  from Russian storages left behind after their occupation of this area      Foreign observers who witnessed the events  including Russian Officers who did not desert their lines  submitted detailed reports proving the genocide to Ottoman commanders who received them as prisoners of war  What is most important is that they stated in their reports  the massacres did not happen by chance but were planned      At the end of the war  the German author Dr  Weiss  his Austrian colleague Dr  Stein and his Turkish colleague Mr  Ahmet Vefik visited Trabzon  Kars  Erzurum and Batum between April   th and May   th      to record the cruelties  Their writings not only show the scope of Armenian activities  but also reveal their goal and true nature          The Ottoman State  the Ministry of War    Islam Ahalinin Ducar Olduklari Mezalim Hakkinda Vesaike Mustenid Malumat    Istanbul         The French version   Documents Relatifs aux Atrocites Commises par les Armeniens sur la Population Musulmane    Istanbul         In the Latin script  H  K  Turkozu  ed    Osmanli ve Sovyet Belgeleriyle Ermeni Mezalimi    Ankara         In addition  Z  Basar  ed    Ermenilerden Gorduklerimiz    Ankara        and  edited by the same author   Ermeniler Hakkinda Makaleler   Derlemeler    Ankara          Askeri Tarih Belgeleri       Vol          December        document numbered            Askeri Tarih Belgeleri        Vol          December        document numbered           From Twerdo Khlebof s report dated    April       quoted in Ermeniler      Vol     p           A  R   Altinay    Iki Komite   Iki Kital    Istanbul         and   Kafkas Yollarinda Hatiralar ve Tahassusler   Istanbul         Serdar Argic</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           content  \\\n",
       "8535  \\nYou could take my wrongly spelled surname :-).\\n\\nCheers,\\nKent Sandvik                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "5114  Last week I asked for help in getting an old homemade amp working with\\nmy Sun CD-ROM drive. It turns out that the channel I was testing with\\nwas burned out in the amp. The other channel works fine.\\n\\nSo now I need a new amplifier chip. My local Radio Shack no longer\\ncarries components! The chip is a 12 pin SIP (?) labelled with BA5406\\nand then \"502 515\" below that.\\n\\nDoes anyone have a source? Thanks,                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     \n",
       "5913  \\n\\nThe theory is that the hollering kills the spirit of the criminal/Nazi \\nArmenians of the ASALA/SDPA/ARF Terrorism and Revisionism Triangle. \\nNow, try dealing with the rest of what I wrote.\\n\\nWhat is more, the activities of the Armenian Government seem to have been\\nefforts aimed at eradicating a race (the Turks) or aimed at carrying out a\\none-sided feud, instead of being a struggle for liberation. From the outset,\\nthe efforts of the Armenian revolutionaries within the Ottoman borders took\\nthe form of terrorist and destructive actions aimed at mass murder, cruelty\\nand genocide, so that no other interpretation of them is possible. Armenian\\nactivities started during the reign of Abdulhamid II as individual acts of\\nterror, and then developed into assassinations and surprise attacks. The element\\nof brute force in these activities increased steadily, culminating in mass\\nrebellions and widespread fighting during the First World War. Furthermore,\\nwhen the Ottoman army withdrew from Eastern Anatolia after the 1915 Sarikamis\\ndefeat, Armenian revolutionaries initiated a series of cruelties in this area.\\nAlthough the Russians occupied Eastern Anatolia as an enemy, nevertheless they\\nwere constrained by the rules of war. However, when they returned to their\\ncountry in 1917 after the Revolution, Armenian revolutionaries were unchecked\\nin this area for about a year until the Ottoman forces returned to Erzurum\\nin 1918. During this period, Armenian revolutionaries executed massacres on\\nthe local people which is recorded in historical documents.[1]\\n\\nFor example, let us look at a report dated 21 March 1918 which the Commander\\nof the Third Army submitted when he entered Erzurum and Erzincan: \\n\\n \"They were completely and systematically destroyed and burned down \\n  by Armenians, even the trees were cut down, and they are like a \\n  building entirely consumed by fire in every sense of the word.\" \\n\\nAs for the people who had been living in Erzurum and Erzincan:\\n\\n\"Those who were capable of fighting were taken away at the very beginning\\n with the excuse of forced labor in road construction, they were taken\\n in the direction of Sarikamis and annihilated. When the Russian army\\n withdrew, a part of the remaining people was destroyed in Armenian\\n massacres and cruelties: they were thrown into wells, they were locked\\n in houses and burned down, they were killed with bayonets and swords, in places\\n selected as butchering spots, their bellies were torn open, their lungs\\n were pulled out, and girls and women were hanged by their hair after\\n being subjected to every conceivable abominable act. A very small part \\n of the people who were spared these abominations far worse than the\\n cruelty of the inquisition resembled living dead and were suffering\\n from temporary insanity because of the dire poverty they had lived\\n in and because of the frightful experiences they had been subjected to.\\n Including women and children, such persons discovered so far do not\\n exceed one thousand five hundred in Erzincan and thirty thousand in\\n Erzurum. All the fields in Erzincan and Erzurum are untilled, everything\\n that the people had has been taken away from them, and we found them\\n in a destitute situation. At the present time, the people are subsisting\\n on some food they obtained, impelled by starvation, from Russian storages\\n left behind after their occupation of this area.\"[2]\\n \\nForeign observers who witnessed the events, including Russian Officers\\nwho did not desert their lines, submitted detailed reports proving the\\ngenocide to Ottoman commanders who received them as prisoners of war.\\nWhat is most important is that they stated in their reports 'the \\nmassacres did not happen by chance but were planned.'[3]\\n\\nAt the end of the war, the German author Dr. Weiss, his Austrian colleague\\nDr. Stein and his Turkish colleague Mr. Ahmet Vefik visited Trabzon, Kars,\\nErzurum and Batum between April 17th and May 20th 1918 to record the\\ncruelties. Their writings not only show the scope of Armenian activities,\\nbut also reveal their goal and true nature.[4]\\n\\n[1] (The Ottoman State, the Ministry of War), \"Islam Ahalinin Ducar Olduklari\\n    Mezalim Hakkinda Vesaike Mustenid Malumat,\" (Istanbul, 1918). The French\\n    version: \"Documents Relatifs aux Atrocites Commises par les Armeniens sur\\n    la Population Musulmane,\" (Istanbul, 1919). In the Latin script: H. K.\\n    Turkozu, ed., \"Osmanli ve Sovyet Belgeleriyle Ermeni Mezalimi,\" (Ankara,\\n    1982). In addition: Z. Basar, ed., \"Ermenilerden Gorduklerimiz,\" (Ankara,\\n    1974) and, edited by the same author, \"Ermeniler Hakkinda Makaleler -\\n    Derlemeler,\" (Ankara, 1978). \"Askeri Tarih Belgeleri ...,\" Vol. 32, 83\\n    (December 1983), document numbered 1881.\\n[2] \"Askeri Tarih Belgeleri ....,\" Vol. 31, 81 (December 1982), document\\n    numbered 1869.\\n[3] From Twerdo-Khlebof's report dated 29 April 1918; quoted in Ermeniler ...,\\n    Vol. 2, p. 275.\\n[4] A. R. (Altinay), \"Iki Komite - Iki Kital,\" (Istanbul, 1919), and, \"Kafkas\\n    Yollarinda Hatiralar ve Tahassusler\" (Istanbul, 1919).\\n\\n\\nSerdar Argic   \n",
       "\n",
       "      target           target_names  \\\n",
       "8535  0       alt.atheism             \n",
       "5114  12      sci.electronics         \n",
       "5913  17      talk.politics.mideast   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                           clean_text  \n",
       "8535  You could take my wrongly spelled surname      Cheers  Kent Sandvik                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "5114  Last week I asked for help in getting an old homemade amp working with my Sun CD ROM drive  It turns out that the channel I was testing with was burned out in the amp  The other channel works fine  So now I need a new amplifier chip  My local Radio Shack no longer carries components  The chip is a    pin SIP     labelled with BA     and then           below that  Does anyone have a source  Thanks                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "5913  The theory is that the hollering kills the spirit of the criminal Nazi Armenians of the ASALA SDPA ARF Terrorism and Revisionism Triangle  Now  try dealing with the rest of what I wrote  What is more  the activities of the Armenian Government seem to have been efforts aimed at eradicating a race  the Turks  or aimed at carrying out a one sided feud  instead of being a struggle for liberation  From the outset  the efforts of the Armenian revolutionaries within the Ottoman borders took the form of terrorist and destructive actions aimed at mass murder  cruelty and genocide  so that no other interpretation of them is possible  Armenian activities started during the reign of Abdulhamid II as individual acts of terror  and then developed into assassinations and surprise attacks  The element of brute force in these activities increased steadily  culminating in mass rebellions and widespread fighting during the First World War  Furthermore  when the Ottoman army withdrew from Eastern Anatolia after the      Sarikamis defeat  Armenian revolutionaries initiated a series of cruelties in this area  Although the Russians occupied Eastern Anatolia as an enemy  nevertheless they were constrained by the rules of war  However  when they returned to their country in      after the Revolution  Armenian revolutionaries were unchecked in this area for about a year until the Ottoman forces returned to Erzurum in       During this period  Armenian revolutionaries executed massacres on the local people which is recorded in historical documents     For example  let us look at a report dated    March      which the Commander of the Third Army submitted when he entered Erzurum and Erzincan   They were completely and systematically destroyed and burned down by Armenians  even the trees were cut down  and they are like a building entirely consumed by fire in every sense of the word   As for the people who had been living in Erzurum and Erzincan   Those who were capable of fighting were taken away at the very beginning with the excuse of forced labor in road construction  they were taken in the direction of Sarikamis and annihilated  When the Russian army withdrew  a part of the remaining people was destroyed in Armenian massacres and cruelties  they were thrown into wells  they were locked in houses and burned down  they were killed with bayonets and swords  in places selected as butchering spots  their bellies were torn open  their lungs were pulled out  and girls and women were hanged by their hair after being subjected to every conceivable abominable act  A very small part of the people who were spared these abominations far worse than the cruelty of the inquisition resembled living dead and were suffering from temporary insanity because of the dire poverty they had lived in and because of the frightful experiences they had been subjected to  Including women and children  such persons discovered so far do not exceed one thousand five hundred in Erzincan and thirty thousand in Erzurum  All the fields in Erzincan and Erzurum are untilled  everything that the people had has been taken away from them  and we found them in a destitute situation  At the present time  the people are subsisting on some food they obtained  impelled by starvation  from Russian storages left behind after their occupation of this area      Foreign observers who witnessed the events  including Russian Officers who did not desert their lines  submitted detailed reports proving the genocide to Ottoman commanders who received them as prisoners of war  What is most important is that they stated in their reports  the massacres did not happen by chance but were planned      At the end of the war  the German author Dr  Weiss  his Austrian colleague Dr  Stein and his Turkish colleague Mr  Ahmet Vefik visited Trabzon  Kars  Erzurum and Batum between April   th and May   th      to record the cruelties  Their writings not only show the scope of Armenian activities  but also reveal their goal and true nature          The Ottoman State  the Ministry of War    Islam Ahalinin Ducar Olduklari Mezalim Hakkinda Vesaike Mustenid Malumat    Istanbul         The French version   Documents Relatifs aux Atrocites Commises par les Armeniens sur la Population Musulmane    Istanbul         In the Latin script  H  K  Turkozu  ed    Osmanli ve Sovyet Belgeleriyle Ermeni Mezalimi    Ankara         In addition  Z  Basar  ed    Ermenilerden Gorduklerimiz    Ankara        and  edited by the same author   Ermeniler Hakkinda Makaleler   Derlemeler    Ankara          Askeri Tarih Belgeleri       Vol          December        document numbered            Askeri Tarih Belgeleri        Vol          December        document numbered           From Twerdo Khlebof s report dated    April       quoted in Ermeniler      Vol     p           A  R   Altinay    Iki Komite   Iki Kital    Istanbul         and   Kafkas Yollarinda Hatiralar ve Tahassusler   Istanbul         Serdar Argic  "
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kSW10Cb33Lzk"
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pkgfv9tc3Lzi"
   },
   "outputs": [],
   "source": [
    "# Leverage tqdm for progress_apply\n",
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# If you're on macOS, Linux, or python session executed from Windows Subsystem for Linux (WSL)\n",
    "# conda activate U4-S1-NLP\n",
    "# pip install pandarallel\n",
    "#\n",
    "# from pandarallel import pandarallel\n",
    "# pandarallel.initialize(progress_bar=True)\n",
    "#\n",
    "# df['lemmas'] = df['content'].parallel_apply(get_lemmas)\n",
    "#\n",
    "# Ref: https://github.com/nalepae/pandarallel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6EBPQXqEKE9P"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11314/11314 [04:47<00:00, 39.32it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create 'lemmas' column\n",
    "def get_lemmas(x):\n",
    "    lemmas = []\n",
    "    for token in nlp(x):\n",
    "        if (token.is_stop!=True) and (token.is_punct!=True):\n",
    "            lemmas.append(token.lemma_)\n",
    "    return lemmas\n",
    "\n",
    "df['lemmas'] = df['clean_text'].progress_apply(get_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-yvsyIpvKBlw"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "      <th>target</th>\n",
       "      <th>target_names</th>\n",
       "      <th>clean_text</th>\n",
       "      <th>lemmas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.</td>\n",
       "      <td>7</td>\n",
       "      <td>rec.autos</td>\n",
       "      <td>I was wondering if anyone out there could enlighten me on this car I saw the other day  It was a   door sports car  looked to be from the late   s  early   s  It was called a Bricklin  The doors were really small  In addition  the front bumper was separate from the rest of the body  This is all I know  If anyone can tellme a model name  engine specs  years of production  where this car is made  history  or whatever info you have on this funky looking car  please e mail</td>\n",
       "      <td>[wonder, enlighten, car, see, day,  ,   , door, sport, car,  , look, late,   , s,  , early,   , s,  , call, Bricklin,  , door, small,  , addition,  , bumper, separate, rest, body,  , know,  , tellme, model,  , engine, spec,  , year, production,  , car,  , history,  , info, funky, look, car,  , e, mail]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>A fair number of brave souls who upgraded their SI clock oscillator have shared their experiences for this poll  Please send a brief message detailing your experiences with the procedure  Top speed attained  CPU rated speed  add on cards and adapters  heat sinks  hour of usage per day  floppy disk functionality with     and     m floppies are especially requested  I will be summarizing in the next two days  so please add to the network knowledge base if you have done the clock upgrade and haven t answered this poll  Thanks</td>\n",
       "      <td>[fair, number, brave, soul, upgrade, SI, clock, oscillator, share, experience, poll,  , send, brief, message, detail, experience, procedure,  , speed, attain,  , cpu, rate, speed,  , add, card, adapter,  , heat, sink,  , hour, usage, day,  , floppy, disk, functionality,     ,     , m, floppy, especially, request,  , summarize, day,  , add, network, knowledge, base, clock, upgrade, haven, t, answer, poll,  , thank]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>well folks, my mac plus finally gave up the ghost this weekend after\\nstarting life as a 512k way back in 1985.  sooo, i'm in the market for a\\nnew machine a bit sooner than i intended to be...\\n\\ni'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\\nof questions that (hopefully) somebody can answer:\\n\\n* does anybody know any dirt on when the next round of powerbook\\nintroductions are expected?  i'd heard the 185c was supposed to make an\\nappearence \"this summer\" but haven't heard anymore on it - and since i\\ndon't have access to macleak, i was wondering if anybody out there had\\nmore info...\\n\\n* has anybody heard rumors about price drops to the powerbook line like the\\nones the duo's just went through recently?\\n\\n* what's the impression of the display on the 180?  i could probably swing\\na 180 if i got the 80Mb disk rather than the 120, but i don't really have\\na feel for how much \"better\" the display is (yea, it looks great in the\\nstore, but is that all \"wow\" or is it really that good?).  could i solicit\\nsome opinions of people who use the 160 and 180 day-to-day on if its worth\\ntaking the disk size and money hit to get the active display?  (i realize\\nthis is a real subjective question, but i've only played around with the\\nmachines in a computer store breifly and figured the opinions of somebody\\nwho actually uses the machine daily might prove helpful).\\n\\n* how well does hellcats perform?  ;)\\n\\nthanks a bunch in advance for any info - if you could email, i'll post a\\nsummary (news reading time is at a premium with finals just around the\\ncorner... :( )\\n--\\nTom Willis  \\  twillis@ecn.purdue.edu    \\    Purdue Electrical Engineering</td>\n",
       "      <td>4</td>\n",
       "      <td>comp.sys.mac.hardware</td>\n",
       "      <td>well folks  my mac plus finally gave up the ghost this weekend after starting life as a    k way back in       sooo  i m in the market for a new machine a bit sooner than i intended to be    i m looking into picking up a powerbook     or maybe     and have a bunch of questions that  hopefully  somebody can answer    does anybody know any dirt on when the next round of powerbook introductions are expected  i d heard the    c was supposed to make an appearence  this summer  but haven t heard anymore on it   and since i don t have access to macleak  i was wondering if anybody out there had more info      has anybody heard rumors about price drops to the powerbook line like the ones the duo s just went through recently    what s the impression of the display on the      i could probably swing a     if i got the   Mb disk rather than the      but i don t really have a feel for how much  better  the display is  yea  it looks great in the store  but is that all  wow  or is it really that good    could i solicit some opinions of people who use the     and     day to day on if its worth taking the disk size and money hit to get the active display   i realize this is a real subjective question  but i ve only played around with the machines in a computer store breifly and figured the opinions of somebody who actually uses the machine daily might prove helpful     how well does hellcats perform     thanks a bunch in advance for any info   if you could email  i ll post a summary  news reading time is at a premium with finals just around the corner            Tom Willis   twillis ecn purdue edu   Purdue Electrical Engineering</td>\n",
       "      <td>[folk,  , mac, plus, finally, give, ghost, weekend, start, life,    , k, way,       , sooo,  , m, market, new, machine, bit, sooner, intend,    , m, look, pick, powerbook,     , maybe,     , bunch, question,  , hopefully,  , somebody, answer,    , anybody, know, dirt, round, powerbook, introduction, expect,  , d, hear,    , c, suppose, appearence,  , summer,  , haven, t, hear, anymore,   , don, t, access, macleak,  , wonder, anybody, info,      , anybody, hear, rumor, price, drop, powerbook, line, like, one, duo, s, go, recently,    , s, impression, display,      , probably, swing,     , get,   , Mb, disk,      , don, t, feel,  , ...]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>\\nDo you have Weitek's address/phone number?  I'd like to get some information\\nabout this chip.\\n</td>\n",
       "      <td>1</td>\n",
       "      <td>comp.graphics</td>\n",
       "      <td>Do you have Weitek s address phone number  I d like to get some information about this chip</td>\n",
       "      <td>[Weitek, s, address, phone, number,  , d, like, information, chip]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>From article &lt;C5owCB.n3p@world.std.com&gt;, by tombaker@world.std.com (Tom A Baker):\\n\\n\\nMy understanding is that the 'expected errors' are basically\\nknown bugs in the warning system software - things are checked\\nthat don't have the right values in yet because they aren't\\nset till after launch, and suchlike. Rather than fix the code\\nand possibly introduce new bugs, they just tell the crew\\n'ok, if you see a warning no. 213 before liftoff, ignore it'.</td>\n",
       "      <td>14</td>\n",
       "      <td>sci.space</td>\n",
       "      <td>From article  C owCB n p world std com   by tombaker world std com  Tom A Baker   My understanding is that the  expected errors  are basically known bugs in the warning system software   things are checked that don t have the right values in yet because they aren t set till after launch  and suchlike  Rather than fix the code and possibly introduce new bugs  they just tell the crew  ok  if you see a warning no      before liftoff  ignore it</td>\n",
       "      <td>[article,  , C, owcb, n, p, world, std, com,   , tombaker, world, std, com,  , Tom, Baker,   , understanding,  , expect, error,  , basically, know, bug, warning, system, software,   , thing, check, don, t, right, value, aren, t, set, till, launch,  , suchlike,  , fix, code, possibly, introduce, new, bug,  , tell, crew,  , ok,  , warning,      , liftoff,  , ignore,  ]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                       content  \\\n",
       "0  I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                             \n",
       "1  A fair number of brave souls who upgraded their SI clock oscillator have\\nshared their experiences for this poll. Please send a brief message detailing\\nyour experiences with the procedure. Top speed attained, CPU rated speed,\\nadd on cards and adapters, heat sinks, hour of usage per day, floppy disk\\nfunctionality with 800 and 1.4 m floppies are especially requested.\\n\\nI will be summarizing in the next two days, so please add to the network\\nknowledge base if you have done the clock upgrade and haven't answered this\\npoll. Thanks.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    \n",
       "2  well folks, my mac plus finally gave up the ghost this weekend after\\nstarting life as a 512k way back in 1985.  sooo, i'm in the market for a\\nnew machine a bit sooner than i intended to be...\\n\\ni'm looking into picking up a powerbook 160 or maybe 180 and have a bunch\\nof questions that (hopefully) somebody can answer:\\n\\n* does anybody know any dirt on when the next round of powerbook\\nintroductions are expected?  i'd heard the 185c was supposed to make an\\nappearence \"this summer\" but haven't heard anymore on it - and since i\\ndon't have access to macleak, i was wondering if anybody out there had\\nmore info...\\n\\n* has anybody heard rumors about price drops to the powerbook line like the\\nones the duo's just went through recently?\\n\\n* what's the impression of the display on the 180?  i could probably swing\\na 180 if i got the 80Mb disk rather than the 120, but i don't really have\\na feel for how much \"better\" the display is (yea, it looks great in the\\nstore, but is that all \"wow\" or is it really that good?).  could i solicit\\nsome opinions of people who use the 160 and 180 day-to-day on if its worth\\ntaking the disk size and money hit to get the active display?  (i realize\\nthis is a real subjective question, but i've only played around with the\\nmachines in a computer store breifly and figured the opinions of somebody\\nwho actually uses the machine daily might prove helpful).\\n\\n* how well does hellcats perform?  ;)\\n\\nthanks a bunch in advance for any info - if you could email, i'll post a\\nsummary (news reading time is at a premium with finals just around the\\ncorner... :( )\\n--\\nTom Willis  \\  twillis@ecn.purdue.edu    \\    Purdue Electrical Engineering   \n",
       "3  \\nDo you have Weitek's address/phone number?  I'd like to get some information\\nabout this chip.\\n                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                            \n",
       "4  From article <C5owCB.n3p@world.std.com>, by tombaker@world.std.com (Tom A Baker):\\n\\n\\nMy understanding is that the 'expected errors' are basically\\nknown bugs in the warning system software - things are checked\\nthat don't have the right values in yet because they aren't\\nset till after launch, and suchlike. Rather than fix the code\\nand possibly introduce new bugs, they just tell the crew\\n'ok, if you see a warning no. 213 before liftoff, ignore it'.                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                      \n",
       "\n",
       "   target           target_names  \\\n",
       "0  7       rec.autos               \n",
       "1  4       comp.sys.mac.hardware   \n",
       "2  4       comp.sys.mac.hardware   \n",
       "3  1       comp.graphics           \n",
       "4  14      sci.space               \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                clean_text  \\\n",
       "0  I was wondering if anyone out there could enlighten me on this car I saw the other day  It was a   door sports car  looked to be from the late   s  early   s  It was called a Bricklin  The doors were really small  In addition  the front bumper was separate from the rest of the body  This is all I know  If anyone can tellme a model name  engine specs  years of production  where this car is made  history  or whatever info you have on this funky looking car  please e mail                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \n",
       "1  A fair number of brave souls who upgraded their SI clock oscillator have shared their experiences for this poll  Please send a brief message detailing your experiences with the procedure  Top speed attained  CPU rated speed  add on cards and adapters  heat sinks  hour of usage per day  floppy disk functionality with     and     m floppies are especially requested  I will be summarizing in the next two days  so please add to the network knowledge base if you have done the clock upgrade and haven t answered this poll  Thanks                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          \n",
       "2  well folks  my mac plus finally gave up the ghost this weekend after starting life as a    k way back in       sooo  i m in the market for a new machine a bit sooner than i intended to be    i m looking into picking up a powerbook     or maybe     and have a bunch of questions that  hopefully  somebody can answer    does anybody know any dirt on when the next round of powerbook introductions are expected  i d heard the    c was supposed to make an appearence  this summer  but haven t heard anymore on it   and since i don t have access to macleak  i was wondering if anybody out there had more info      has anybody heard rumors about price drops to the powerbook line like the ones the duo s just went through recently    what s the impression of the display on the      i could probably swing a     if i got the   Mb disk rather than the      but i don t really have a feel for how much  better  the display is  yea  it looks great in the store  but is that all  wow  or is it really that good    could i solicit some opinions of people who use the     and     day to day on if its worth taking the disk size and money hit to get the active display   i realize this is a real subjective question  but i ve only played around with the machines in a computer store breifly and figured the opinions of somebody who actually uses the machine daily might prove helpful     how well does hellcats perform     thanks a bunch in advance for any info   if you could email  i ll post a summary  news reading time is at a premium with finals just around the corner            Tom Willis   twillis ecn purdue edu   Purdue Electrical Engineering   \n",
       "3  Do you have Weitek s address phone number  I d like to get some information about this chip                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               \n",
       "4  From article  C owCB n p world std com   by tombaker world std com  Tom A Baker   My understanding is that the  expected errors  are basically known bugs in the warning system software   things are checked that don t have the right values in yet because they aren t set till after launch  and suchlike  Rather than fix the code and possibly introduce new bugs  they just tell the crew  ok  if you see a warning no      before liftoff  ignore it                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                              \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                               lemmas  \n",
       "0  [wonder, enlighten, car, see, day,  ,   , door, sport, car,  , look, late,   , s,  , early,   , s,  , call, Bricklin,  , door, small,  , addition,  , bumper, separate, rest, body,  , know,  , tellme, model,  , engine, spec,  , year, production,  , car,  , history,  , info, funky, look, car,  , e, mail]                                                                                                                                                                                                                                                                                                                                                     \n",
       "1  [fair, number, brave, soul, upgrade, SI, clock, oscillator, share, experience, poll,  , send, brief, message, detail, experience, procedure,  , speed, attain,  , cpu, rate, speed,  , add, card, adapter,  , heat, sink,  , hour, usage, day,  , floppy, disk, functionality,     ,     , m, floppy, especially, request,  , summarize, day,  , add, network, knowledge, base, clock, upgrade, haven, t, answer, poll,  , thank]                                                                                                                                                                                                                                   \n",
       "2  [folk,  , mac, plus, finally, give, ghost, weekend, start, life,    , k, way,       , sooo,  , m, market, new, machine, bit, sooner, intend,    , m, look, pick, powerbook,     , maybe,     , bunch, question,  , hopefully,  , somebody, answer,    , anybody, know, dirt, round, powerbook, introduction, expect,  , d, hear,    , c, suppose, appearence,  , summer,  , haven, t, hear, anymore,   , don, t, access, macleak,  , wonder, anybody, info,      , anybody, hear, rumor, price, drop, powerbook, line, like, one, duo, s, go, recently,    , s, impression, display,      , probably, swing,     , get,   , Mb, disk,      , don, t, feel,  , ...]  \n",
       "3  [Weitek, s, address, phone, number,  , d, like, information, chip]                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  \n",
       "4  [article,  , C, owcb, n, p, world, std, com,   , tombaker, world, std, com,  , Tom, Baker,   , understanding,  , expect, error,  , basically, know, bug, warning, system, software,   , thing, check, don, t, right, value, aren, t, set, till, launch,  , suchlike,  , fix, code, possibly, introduce, new, bug,  , tell, crew,  , ok,  , warning,      , liftoff,  , ignore,  ]                                                                                                                                                                                                                                                                                   "
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cPuCuK0z3Lzs"
   },
   "source": [
    "### The two main inputs to the LDA topic model are the dictionary (id2word) and the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1klqRpqtJxWc"
   },
   "outputs": [],
   "source": [
    "# Create Dictionary\n",
    "id2word = corpora.Dictionary(df['lemmas'] )\n",
    "\n",
    "# Term Document Frequency\n",
    "corpus = [id2word.doc2bow(text) for text in df['lemmas']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77754"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many words do we have?\n",
    "len(id2word.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's remove extreme values from the dataset\n",
    "id2word.filter_extremes(no_below=5, no_above=0.75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14778"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many words do we have?\n",
    "len(id2word.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ArZPxcP5LH1J"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sheet'"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_IIBnI2e3Lzw"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\n\\n\\n\\nOf course.  The term must be rigidly defined in any bill.\\n\\n\\nI doubt she uses this term for that.  You are using a quote allegedly\\nfrom her, can you back it up?\\n\\n\\n\\n\\nI read the article as presenting first an argument about weapons of mass\\ndestruction (as commonly understood) and then switching to other topics.\\nThe first point evidently was to show that not all weapons should be\\nallowed, and then the later analysis was, given this understanding, to\\nconsider another class.\\n\\n\\n\\n'"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bGFG9E2j3Lzz"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(0, 11),\n",
       " (117, 1),\n",
       " (177, 1),\n",
       " (193, 1),\n",
       " (221, 1),\n",
       " (225, 1),\n",
       " (226, 1),\n",
       " (227, 1),\n",
       " (228, 1),\n",
       " (229, 1),\n",
       " (230, 1),\n",
       " (231, 1),\n",
       " (232, 1),\n",
       " (233, 1),\n",
       " (234, 1),\n",
       " (235, 1),\n",
       " (236, 1),\n",
       " (237, 1),\n",
       " (238, 1),\n",
       " (239, 1),\n",
       " (240, 1),\n",
       " (241, 1),\n",
       " (242, 1),\n",
       " (243, 1),\n",
       " (244, 1),\n",
       " (245, 1),\n",
       " (246, 2),\n",
       " (247, 1),\n",
       " (248, 1),\n",
       " (249, 2)]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TB6wox963Lz2"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'rm'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[252]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kk3g75XX3Lz4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'controller'"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "id2word[276]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6K2jWxHJLOzK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('  ', 11),\n",
       " ('helpful', 1),\n",
       " ('address', 1),\n",
       " ('ignore', 1),\n",
       " ('course', 1),\n",
       " ('evidently', 1),\n",
       " ('later', 1),\n",
       " ('mass', 1),\n",
       " ('point', 1),\n",
       " ('present', 1),\n",
       " ('quote', 1),\n",
       " ('read', 1),\n",
       " ('switch', 1),\n",
       " ('term', 1),\n",
       " ('topic', 1),\n",
       " ('understand', 1),\n",
       " ('weapon', 1),\n",
       " ('News', 1),\n",
       " ('Sean', 1),\n",
       " ('September', 1),\n",
       " ('Sharon', 1),\n",
       " ('accidentally', 1),\n",
       " ('bounce', 1),\n",
       " ('couldn', 1),\n",
       " ('delete', 1),\n",
       " ('directly', 1),\n",
       " ('file', 2),\n",
       " ('glad', 1),\n",
       " ('instead', 1),\n",
       " ('prob', 2)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Human readable format of corpus (term-frequency)\n",
    "[(id2word[word_id], word_count) for word_id, word_count in corpus[5]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "le-XzI923Lz8"
   },
   "source": [
    "# Part 2: Estimate a LDA Model with Gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AlNG4bSI3Lz8"
   },
   "source": [
    " ### Train an LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Fasvjf0VLQ2a"
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 14778 is out of bounds for axis 1 with size 14778",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\U4-S1-NLP\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics, callbacks, dtype)\u001b[0m\n\u001b[0;32m    517\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 519\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    520\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\U4-S1-NLP\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[0;32m    978\u001b[0m                         \u001b[0mpass_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunk_no\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    979\u001b[0m                     )\n\u001b[1;32m--> 980\u001b[1;33m                     \u001b[0mgammat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdo_estep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mother\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    981\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimize_alpha\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\U4-S1-NLP\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mdo_estep\u001b[1;34m(self, chunk, state)\u001b[0m\n\u001b[0;32m    740\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstate\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    741\u001b[0m             \u001b[0mstate\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 742\u001b[1;33m         \u001b[0mgamma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msstats\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcollect_sstats\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    743\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msstats\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0msstats\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    744\u001b[0m         \u001b[0mstate\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumdocs\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m  \u001b[1;31m# avoids calling len(chunk) on a generator\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\.conda\\envs\\U4-S1-NLP\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[1;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[0;32m    678\u001b[0m             \u001b[0mElogthetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mElogtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    679\u001b[0m             \u001b[0mexpElogthetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexpElogtheta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 680\u001b[1;33m             \u001b[0mexpElogbetad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpElogbeta\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    681\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    682\u001b[0m             \u001b[1;31m# The optimal phi_{dwk} is proportional to expElogthetad_k * expElogbetad_w.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: index 14778 is out of bounds for axis 1 with size 14778"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "lda_model = gensim.models.ldamodel.LdaModel(corpus=corpus,\n",
    "                                            id2word=id2word,\n",
    "                                            num_topics=20, \n",
    "                                            chunksize=100,\n",
    "                                            passes=10,\n",
    "                                            per_word_topics=True)\n",
    "\n",
    "# https://radimrehurek.com/gensim/models/ldamodel.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "49CabmIj3Lz_"
   },
   "outputs": [],
   "source": [
    "# lda_model.save('lda_model.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XDgUshRE3L0C"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "lda_multicore = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                                        id2word=id2word,\n",
    "                                                        num_topics=20, \n",
    "                                                        chunksize=100,\n",
    "                                                        passes=10,\n",
    "                                                        per_word_topics=True,\n",
    "                                                        workers=12)\n",
    "\n",
    "# https://radimrehurek.com/gensim/models/ldamulticore.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Rgb9AaPq3L0E"
   },
   "outputs": [],
   "source": [
    "lda_multicore.save('lda_multicore.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VLxvJPoK3L0G"
   },
   "outputs": [],
   "source": [
    "from gensim import models\n",
    "lda_multicore =  models.LdaModel.load('lda_multicore.model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uDH3tzx13L0I"
   },
   "source": [
    "### View the topics in LDA model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "newsgroups_train.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JawR8yscLVNS"
   },
   "outputs": [],
   "source": [
    "pprint(lda_multicore.print_topics())\n",
    "doc_lda = lda_multicore[corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distro = [lda[d] for d in corpus]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gKEP0bIC3L0K"
   },
   "source": [
    "### What is topic Perplexity?\n",
    "Perplexity is a statistical measure of how well a probability model predicts a sample. As applied to LDA, for a given value of , you estimate the LDA model. Then given the theoretical word distributions represented by the topics, compare that to the actual topic mixtures, or distribution of words in your documents.\n",
    "\n",
    "### What is topic coherence?\n",
    "Topic Coherence measures score a single topic by measuring the degree of semantic similarity between high scoring words in the topic. These measurements help distinguish between topics that are semantically interpretable topics and topics that are artifacts of statistical inference.\n",
    "A set of statements or facts is said to be coherent, if they support each other. Thus, a coherent fact set can be interpreted in a context that covers all or most of the facts. An example of a coherent fact set is â€œthe game is a team sportâ€, â€œthe game is played with a ballâ€, â€œthe game demands great physical effortsâ€"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xCjhr8k4LXSy"
   },
   "outputs": [],
   "source": [
    "# Compute Perplexity\n",
    "print('\\nPerplexity: ', lda_multicore.log_perplexity(corpus))  # a measure of how good the model is. lower the better.\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_lda = CoherenceModel(model=lda_multicore, \n",
    "                                     texts=df['lemmas'], \n",
    "                                     dictionary=id2word, \n",
    "                                     coherence='c_v')\n",
    "coherence_lda = coherence_model_lda.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_lda)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7UvfgYt93L0X"
   },
   "source": [
    "# Part 3: Interpret LDA results & Select the appropriate number of topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CYXi480VLaHK"
   },
   "outputs": [],
   "source": [
    "pyLDAvis.enable_notebook()\n",
    "vis = pyLDAvis.gensim.prepare(lda_multicore, corpus, id2word)\n",
    "pyLDAvis.display(vis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rjtXk8J3LaXC"
   },
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.ldamulticore.LdaMulticore(corpus=corpus,\n",
    "                                                        id2word=id2word,\n",
    "                                                        num_topics=num_topics, \n",
    "                                                        chunksize=100,\n",
    "                                                        passes=10,\n",
    "                                                        per_word_topics=True,\n",
    "                                                        workers=12)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pRA9EjvQ3L0c"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model_list, coherence_values = compute_coherence_values(dictionary=id2word, \n",
    "                                                        corpus=corpus, \n",
    "                                                        texts=df['lemmas'], \n",
    "                                                        start=2, \n",
    "                                                        limit=40, \n",
    "                                                        step=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2e9X-Ql-3L0e"
   },
   "outputs": [],
   "source": [
    "coherence_values = [0.5054, 0.5332, 0.5452, 0.564, 0.5678, 0.5518, 0.519]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ybEBYQNF3L0g"
   },
   "outputs": [],
   "source": [
    "limit=40; start=2; step=6;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DmvQ2zKZ3L0i"
   },
   "outputs": [],
   "source": [
    "# Print the coherence scores\n",
    "for m, cv in zip(x, coherence_values):\n",
    "    print(\"Num Topics =\", m, \" has Coherence Value of\", round(cv, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yol5vG3R3L0k"
   },
   "outputs": [],
   "source": [
    "# Select the model and print the topics\n",
    "#optimal_model = model_list[4]\n",
    "optimal_model =  models.LdaModel.load('optimal_model.model')\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "pprint(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8HwVEUDI3L0m"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "DSPT6_LS_DS_414_Topic_Modeling.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "U4-S1-NLP",
   "language": "python",
   "name": "u4-s1-nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
